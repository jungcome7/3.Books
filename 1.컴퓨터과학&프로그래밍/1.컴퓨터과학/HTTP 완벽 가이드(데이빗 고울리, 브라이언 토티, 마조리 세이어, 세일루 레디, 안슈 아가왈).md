# HTTP 완벽 가이드
### 지은이 : 데이빗 고울리, 브라이언 토티, 마조리 세이어, 세일루 레디, 안슈 아가왈
### 옮긴이 : 이응준, 정상일
### 출판사 : 인사이트(O'REILLY)
### 읽은 날 : 2020.03.21 ~

# 목차
# Part 1. HTTP: 웹의 기초
### 1. HTTP 개관
### 2. URL과 리소스
### 3. HTTP 메시지
### 4. 커넥션 관리

# Part 2. HTTP 아키텍처
### 5. 웹 서버
### 6. 프락시
### 7. 캐시
### 8. 통합점 : 게이트웨이, 터널, 릴레이
### 9. 웹 로봇
### 10. HTTP/2.0

# Part 3. 식별, 인가, 보안
### 11. 클라이언트 식별과 쿠키
### 12. 기본 인증
### 13. 다이제스트 인증
### 14. 보안 HTTP

# Part 4. 엔터티, 인코딩, 국제화
### 15. 엔터티와 인코딩
### 16. 국제화
### 17. 내용 협상과 트랜스코딩

# Part 5. 콘텐츠 발행 및 배포
### 18. 웹 호스팅
### 19. 배포 시스템
### 20. 리다이렉션과 부하 균형
### 21. 로깅과 사용 추적

# Part 6. 부록
### 부록 A. URI 스킴
### 부록 B. HTTP 상태 코드
### 부록 C. HTTP 헤더 레퍼런스
### 부록 D. MIME 타입
### 부록 E. base-64 인코딩
### 부록 F. 다이제스트 인증
### 부록 G. 언어 태그
### 부록 H. MIME 문자집합 등록

# Part 1. HTTP: 웹의 기초
### 1. HTTP 개관
1. 웹 서버는 모든 HTTP 객체 데이터에 MIME 타입을 붙인다. 웹브라우저는 서버로부터 객체를 돌려받을 때, 다룰 수 있는 객체인지 MIME 타입을 통해 확인한다. 대부분의 웹브라우저는 잘 알려진 객체 타입 수백 가지를 다룰 수 잇다. 이미지 파일을 보여주고, HTML 파일을 분석하거나 포맷팅하고, 오디오 파일을 컴퓨터의 스피커를 통해 재생하고, 특별한 포맷의 파일을 다루기 위해 외부 플러그인 소프트웨어를 실행한다.

2. 서버 리소스 이름은 통합 자원 식별자(uniform resource identifier), 혹은 URI로 불린다.

3. URI에는 두 가지가 있는데, URL과 URN이라는 것이다. 

### 2. URL과 리소스
4. 일반적으로 HTTP 서버는 객체 일부가 아닌 전체만 다루기 때문에, 클라이언트는 서버에 프래그먼트를 전달하지 않는다. 브라우저가 서버로부터 전체 리소스를 내려받은 후, 프래그먼트를 사용하여 당신이 보고자 하는 리소스의 일부를 보여준다.

5. 인코딩 체계  
안전한 문자 집합을 이용하는 경우 그 표현의 한계를 넘기 위해, URL에 있는 안전하지 않은 문자들을 표현할 수 있는 인코딩 방식이 고안되었다. 인코딩은 안전하지 않은 문자를 퍼센티지 기호(%)로 시작해, ASCII 코드로 표현되는 두 개의 16진수 숫자로 이루어진 ‘이스케이프’ 문자로 바꾼다.

### 3. HTTP 메시지
6. HTTP는 인바운드와 아웃바운드라는 용어를 트랜잭션 방향을 표현하기 위해 사용한다. 메시지가 원 서버로 향하는 것은 인바운드로 이동하는 것이고, 모든 처리가 끝난 뒤에 메시지가 사용자 에이전트로 돌아오는 것은 아웃바운드로 이동하는 것이다.

7. 메시지는 결코 업스트림으로 흐르지 않는다.

8. HTTP 헤더는 다음과 같이 분류된다.
- 일반 헤더 : 요청과 응답 양쪽에 모두 나타날 수 있음
- 요청 헤더 : 요청에 대한 부가 정보를 제공
- 응답 헤더 : 응답에 대한 부가 정보를 제공
- Entity 헤더 : 본문 크기와 콘텐츠, 혹은 리소스 그 자체를 서술
- 확장 헤더 : 명세에 정의되지 않은 새로운 헤더

9. 안전한 메서드(Safe Method)  
HTTP는 안전한 메서드라 불리는 메서드의 집합을 정의한다. GET과 HEAD 메서드는 안전하다고 할 수 있는데, 이는 GET이나 HEAD 메서드를 사용하는 HTTP 요청의 결과로 서버에 어떤 작용도 없음을 의미한다.

10. HEAD  
HEAD 메서드는 정확히 GET처럼 행동하지만, 서버는 응답으로 헤더만을 돌려준다. 엔터티 본문은 결코 반환하지 않는다. 이는 클라이언트가 리소스를 실제로 가져올 필요 없이 헤더만을 조사할 수 있도록 해준다. HEAD를 사용ㅇ하면,
- 리소스를 가져오지 않고도 그에 대해 무엇인가(타입이라거나)를 알아낼 수 있다.
- 응답의 상태 코드를 통해, 개체가 존재하는지 확인할 수 있다.
- 헤더를 확인하여 리소스가 변경되었는지 검사할 수 있다.

11. PUT 메서드의 의미는, 서버가 요청의 본문을 가지고 요청 URL의 이름대로 새 문서를 만들거나, 이미 URL이 존재한다면 본문을 상요해서 교체하는 것이다.

12. TRACE 메서드는 주로 진단을 위해 사용된다. 예를 들면 요청이 의도한 요청/응답 연쇄를 거쳐가는지 검사할 수 있다. 또한 프락시나 다른 애플리케이션들이 요청에 어떤 영향을 미치는지 확인해보고자 할 때도 좋은 도구다.

13. OPTIONS  
OPTIONS 메서드는 웹 서버에게 여러 가지 종류의 지원 범위에 대해 물어본다. 서버에게 특정 리소스에 대해 어떤 메서드가 지원되는지 물어볼 수 있다(몇몇 서버는 특정 종류의 객체에 대해 특정 동작만을 지원한다).

14. 확장 메서드(그리고 대부분의 HTTP 확장)를 다룰 때는 “엄격하게 보내고 관대하게 받아들여라”라는 오랜 규칙에 따르는 것이 가장 좋다.
 
### 4. 커넥션 관리
15. 신뢰할 수 있는 데이터 전송 통로인 TCP  
HTTP 커넥션은 몇몇 사용 규칙을 제외하고는 TCP 커넥션에 불과하다. TCP 커넥션은 인터넷을 안정적으로 연결해준다. 신속 정확하게 데이터를 보내고자 한다면 TCP의 기초적은 내용을 알아야 한다.

16. TCP 커넥션은 네 가지 값으로 식별한다.
<발신지 IP주소, 발신지 포트, 수신지 IP주소, 수신지 포트>
이 네 가지 값으로 유일한 커넥션을 생성한다. 서로 다른 두 개의 TCP 커넥션은 네 가지 주소 구성요소의 값이 모두 같을 수 없다(하지만 주소 구성요소 일부가 같을 수는 있다).

17. 소켓 API는 HTTP 프로그래머에게 TCP와 IP 세부사항들을 숨긴다. 소켓 API는 유닉스 운영체제용으로 먼저 개발되었지만, 지금은 소켓 API의 다양한 구현체들 덕분에 대부분의 운영체제와 프로그램 언어에서 이를 사용할 수 있게 되었다.

18. 병렬 커넥션  
앞서 언급했듯이 브라우저는 HTML 페이지에 이어 첫 번째 첨부된 객체, 두 번째 첨부된 객체를 하나씩 내려받는 식으로 웹페이지를 보여줄 수 있다. 하지만 이 방식은 너무 느리다!
HTTP는 클라이언트가 여러 개의 커넥션을 맺음으로써 여러 개의 HTTP 트랜잭션을 병렬로 처리할 수 있게 한다.

19. 백 명의 가상 사용자가 각각 100개의 커넥션을 맺고 있다면, 서버는 총 10,000개의 커넥션을 떠안게 되는 것이다. 이는 서버의 성능을 크게 떨어뜨린다. 이런 상황은 고부하 프락시에서도 동일하게 발생할 수 있다.
브라우저는 실제로 병렬 커넥션을 사용하긴 하지만 적은 수(대부분 6~8개)의 병렬 커넥션만을 허용한다. 서버는 특정 클라이언트로부터 과도한 수의 커넥션이 맺어졌을 경우, 그것을 임의로 끊어버릴 수 있다.

20. 지속 커넥션  
웹 클라이언트는 보통 같은 사이트에 여러 개의 커넥션을 맺는다. 예를 들어 웹페이지에 첨부된 이미지들 대부분은 같은 웹 사이트에 있고, 상당수의 하이퍼링크도 같은 사이트를 가리킨다.  따라서 서버에 HTTP 요청을 하기 시작한 애플리케이션은 웹페이지 내의 이미지 등을 가져오기 위해서 그 서버에 또 요청하게 될 것이다. 이 속성을 사이트 지역성(site locality)라 부른다.

21. 따라서 HTTP/1.1(HTTP/1.0의 개선 버전)을 지원하는 기기는 처리가 완료된 후에도 TCP 커넥션을 유지하여 앞으로 있을 HTTP 요청에 재사용할 수 있다. 처리가 완료된 후에도 계속 연결된 상태로 있는 TCP 커넥션을 지속 커넥션이라 부른다. 비지속 커넥션은 각 처리가 끝날 때마다 커넥션을 끊지만, 지속 커넥션은 클라이언트나 서버가 커넥션을 끊기 전까지는 트랜잭션 간에도 커넥션을 유지한다.

22. 해당 서버에 이미 맺어져 있는 지속 커넥션을 재사용함으로써, 커넥션을 맺기 위한 준비작업에 따르는 시간을 절약할 수 있다. 게다가 이미 맺어져 있는 커넥션은 TCP의 느린시작으로 인한 지연을 피함으로써 더 빠르게 데이터를 전송할 수 있다.

23. Keep-Alive와 멍청한(dumb) 프락시  
특히 문제는 프락시에서 시작되는데, 프락시는 Connection 헤더를 이해하지 못해서 해당 헤더들을 삭제하지 않고 요청 그대로 다음 프락시에 전달한다. 오래되고 단순한 수많은 프락시들이 Connection 헤더에 대한 처리 없이 요청을 그대로 전달한다. 웹 클라이언트가 무조건 전달을 하는 멍청한 프락시를 거쳐 웹 서버에 메시지를 전송한다고 생각해보자.

24. HTTP/1.1의 지속 커넥션  
HTTP/1.1에서는 keep-alive 커넥션을 지원하지 않는 대신, 설계가 더 개선된 지속 커넥션을 지원한다. 지속 커넥션의 목적은 kepp-alive 커넥션과 같지만 그에 비해 더 잘 동작한다.
HTTP/1.0의 keep-alive 커넥션과는 달리 HTTP/1.1의 지속 커넥션은 기본으로 활성화되어 있다. HTTP/1.1에서는 별도 설정을 하지 않는 한, 모든 커넥션을 지속 커넥션으로 취급한다. HTTP/1.1 애플리케이션은 트랜잭션이 끝난 다음 커넥션을 끊으려면 Connection : close 헤더를 명시해야 한다.

25. 한 번 혹은 여러 번 실행됐는지에 상관없이 같은 결과를 반환한다면 그 트랜잭션은 멱등(idempotent)하다고 한다. GET, HEAD, PUT, DELETE, TRACE 그리고 OPTIONS 메서드들은 멱등하다고 이해하면 된다. 클라이언트는 POST와 같이 멱등이 아닌 요청은 파이프라인을 통해 요청하면 안 된다. 그렇지 않으면 전송 커넥션이 예상치 못하게 끊어져 버렸을 때, 알 수 없는 결과를 초래할 수 있다. 비멱등인 요청을 다시 보내야 한다면, 이전 요청에 대한 응답을 받을 때까지 기다려야 한다.

# Part 2. HTTP 아키텍처
### 5. 웹 서버

### 6. 프락시
26. HTTP 프락시 서버는 웹 서버이기도 하고 웹 클라이언트이기도 하다. 프락시는 HTTP 클라이언트의 요청을 받게 되므로, 반드시 웹 서버처럼 요청과 커넥션을 적절히 다루고 응답을 돌려줘야 한다. 동시에 프락시는 요청을 서버로 보내기도 하므로, 요청을 보내고 응답을 받는 올바른 HTTP 클라이언트처럼 동작해야 한다.

27. 엄밀하게 말하면, 프락시는 같은 프로토콜을 사용하는 둘 이상의 애플리케이션을 연결하고, 게이트웨이는 서로 다른 프로토콜을 사용하는 둘 이상을 연결한다. 게이트웨이는 클라이언트와 서버가 서로 다른 프로토콜로 말하더라도 서로 간의 트랜잭션을 완료할 수 있도록 해주는 프로토콜 변환기처럼 동작한다.

28. 프락시 서버는 실용적이고 유용한 것이라면 무슨 일이든 한다. 보안을 개선하고, 성능을 높여주며, 비용을 절약한다. 그리고 프락시 서버는 모든 HTTP 트래픽을 들여다보고 건드릴 수 있기 때문에, 프락시는 부가적인 가치를 주는 여러 유용한 웹서비스를 구현하기 위해 트래픽을 감시하고 수정할 수 있다.

29. 네트워크 보안 엔지니어는 종종 보안을 강화하기 위해 프락시 서버를 사용한다. 프락시 서버는 조직 안에 들어오거나 나가는 응용 레벨 프로토콜의 흐름을 네트워크의 한 지점에서 통제한다. 또한 바이러스를 제거하는 웹이나 이메일 프락시가 사용할 수 있는, 트래픽을 세심히 살펴볼 수 있는 후크(hook)를 제공한다.

30. 웹 캐시  
프락시 캐시는 인기 있는 문서의 로컬 사본을 관리하고 해당 문서에 대한 요청이 오면 빠르게 제공하여, 느리고 비싼 인터넷 커뮤니케이션을 줄인다.

31. 대리 프락시(Surrogate)  
어떤 프락시들은 웹 서버인 것처럼 위장한다. 그렇기 때문에 대리 혹은 리버스 프락시로 불리는 이들은 진짜 웹 서버 요청을 받지만 웹 서버와는 달리 요청 받은 콘텐츠의 위치를 찾아내기 위해 다른 서버와의 커뮤니케이션을 시작한다.
 대리 프락시는 공용 콘텐츠에 대한 느린 웹 서버의 성능을 개선하기 위해 사용될 수 있다. 이런 식으로 사용하는 대리 프락시를 흔히 서버 가속기라고 부른다. 대리 프락시는 또한 콘텐츠 라우팅 기능과 결합되어 주문형 복제 콘텐츠의 분산 네트워크를 만들기 위해 사용될 수 있다.

32. 만약 당신이 명시적인 프락시를 사용한다면, 브라우저는 이와 같이 편리한 확장들 중 어느 것도 더 이상 수행할 수 없다. 브라우저의 URI가 프락시를 그냥 지나쳐버리기 때문이다.

33. 오늘날, 웹 요청이 클라이언트에서 서버로 향하는 도중에 둘 이상의 프락시를 지나게 되는 것은 드문 일이 아니다. 예를 들어, 많은 회사들이 보안과 비용절감을 위해 인터넷 접속 시 캐시 프락시 서버를 사용하며, 많은 대형 ISP들이 성능 개선과 기능 구현을 위해 프락시 캐시를 사용한다. 오늘날 웹 요청의 상당수가 프락시를 지나간다. 동시에 성능상의 이유로 세계 곳곳에 흩어져 있는 대리 캐시 저장고에 콘텐츠를 복제해두는 방식이 점점 더 흔해지고 있다.

34. 프락시 서버는 메시지가 전달될 때 메시지를 바꿀 수 있다. 헤더가 추가되거나, 변경되거나, 삭제될 수 있으며, 본문이 다른 형식으로 변환될 수 있다. 프락시가 점점 복잡해지고 더 많은 벤더가 프락시 제품을 배치하면서, 상호운용성 문제가 증가한다. 프락시 네트워크를 쉽게 진단하기 위해, 우리는 HTTP 프락시 네트워크를 통해 홉에서 홉으로 전달될 때마다 메시지의 내용이 어떻게 변하는지 편리하게 관찰할 방법이 필요하다.
 HTTP/1.1 TRACE 메서드는 요청 메시지를 프락시의 연쇄를 따라가면서 어던 프락시를 지나가고 어떻게 각 프락시가 요청 메시지를 수정하는지 관찰/추적할 수 있도록 해준다. TRACE는 프락시 흐름을 디버깅하는데 매우 유용하다.

### 7. 캐시
35. 캐시  
- 캐시는 불필요한 데이터 전송을 줄여서, 네트워크 요금으로 인한 비용을 줄여준다.
- 캐시는 네트워크 병목을 줄여준다. 대역폭을 늘리지 않고도 페이지를 빨리 불러 올 수 있게 된다.
- 캐시는 원 서버에 대한 요청을 줄여준다. 서버는 부하를 줄일 수 있으며 더 빨리 응답할 수 있게 된다.
- 페이지를 먼 곳에서 불러올수록 시간이 많이 걸리는데, 캐시는 거리로 인한 지연을 줄여준다.

36. 캐시는 또한 네트워크 병목을 줄여준다. 많은 네트워크가 원격 서버보다 로컬 네트워크 클라이언트에 더 넓은 대역폭을 제공한다. 클라이언트들이 서버에 접근할 때의 속도는, 그 경로에 있는 가장 느린 네트워크의 속도와 같다. 만약 클라이언트가 빠른 LAN에 있는 캐시로부터 사본을 가져온다면, 캐싱은 성능을 대폭 개선할 수 있을 것이다(특히 큰 문서들에 대해서).

37. 캐시 재검사(Revalidation)  
… 이를 재검사 적중 혹은 느린 적중이라고 부른다. 이것은 순수 캐시 적중보다 느린데, 원 서버와 검사를 할 필요가 있기 때문이다. 그러나 캐시 부적중보다는 빠른데, 서버로부터 객체 데이터를 받아올 필요가 없기 때문이다.

38. 적중률  
캐시가 요청을 처리하는 비율을 캐시 적중률(혹은 캐시 적중비), 혹은 문서 적중률(혹은 문서 적중비)이라고 부르기도 한다. 적중률은 0에서 1까지의 값으로 되어 있지만, 흔히 퍼센트로 표현되기도 한다. 0%는 모든 요청이 캐시 부적중(네트워크 너머로 문서를 가져와야 했던 경우)임을, 그리고 100%는 모든 요청이 캐시 적중(캐시에서 사본을 가져온 경우)임을 의미한다.
 캐시 관리자는 캐시 적중률이 100%에 근접하게 되는 것을 좋아할 것이다. 실제 적중률은 캐시가 얼마나 큰지, 캐시 사용자들의 관심사가 얼마나 비슷한지, 캐시된 데이터가 얼마나 자주 변경되거나 개인화되는지, 캐시가 어떻게 설정되어 있는지에 달려있다. 적중률은 예측하기 어려운 것으로 악명이 높지만 오늘날 적중률 40%면 웹 캐시로 괜찮은 편이다. 다행인 사실은, 보통 크기의 캐시라도 충분한 분량의 자주 쓰이는 문서들을 보관하여 상당히 트래픽을 줄이고 성능을 개선할 수 있다는 점이다. 캐시는 유용한 콘텐츠가 캐시 안에 머무르도록 보장하기 위해 노력한다.

39. 캐시 토폴로지  
캐시는 한 명의 사용자에게만 할당될 수도 있고 반대로 수천 명의 사용자들 간에 공유될 수도 있다. 한 명에게만 할당된 캐시를 개인 전용 캐시(private cache)라 부른다. 개인 전용 캐시는 개인만을 위한 캐시이므로, 한 명의 사용자가 자주 찾는 페이지를 담는다. 공유된 캐시는 공용 캐시(public cache)라고 불린다. 공용 캐시는 사용자 집단에게 자주 쓰이는 페이지를 담는다.

40. 웹브라우저는 개인 전용 캐시를 내장하고 있다. 대부분의 브라우저는 자주 쓰이는 문서를 개인용 컴퓨터의 디스크와 메모리에 캐시해 놓고, 사용자가 캐시 사이즈와 설정을 수정할 수 있도록 허용한다. 캐시에 어떤 것들이 들어있는지 확인하기 위해 브라우저 안을 들여다보는 것도 가능하다.

41. 공용 캐시는 캐시 프락시 서버 혹은 더 흔히 프락시 캐시라고 불리는 특별한 종류의 공유된 프락시 서버다. 프락시 캐시는 로컬 캐시에서 문서를 제공하거나, 혹은 사용자의 입장에서 서버에 접근한다. 공용 캐시에는 여러 사용자가 접근하기 때문에, 불필요한 트래픽을 줄일 수 있는 더 많은 기회가 있다.

42. HTTP는 어떤 캐시가 사본을 갖고 있는지 서버가 기억하지 않더라도, 캐시된 사본이 서버와 충분히 일치하도록 유지할 수 있게 해주는 단순한 메커니즘을 갖고 있다. HTTP는 이 단순한 메커니즘을 문서 만료와 서버 재검사라고 부른다.

43. 문서 만료
HTTP는 Cache-Control과 Expires라는 특별한 헤더들을 이용해서 원 서버가 각 문서에 유효기간을 붙일 수 있게 해준다.

44. 조건부 메서드와의 재검사
HTTP의 조건부 메서드는 재검사를 효율적으로 만들어준다. HTTP는 캐시가 서버에게 ‘조건부 GET’이라는 요청을 보낼 수 있도록 해준다. 이 요청은 서버가 갖고 있는 문서가 캐시가 갖고 있는 것과 다른 경우에만 객체 본문을 보내달라고 하는 것이다. 이런 식으로, 신선도 검사와 객체를 받아오는 것은 하나의 조건부 GET으로 결합된다. 조건부 GET은 GET 요청 메시지에 특별한 조건부 헤더를 추가함으로써 시작된다. 웹 서버는 조건이 참인 경우에만 객체를 반환한다.
 HTTP는 다섯 가지 조건부 요청 헤더를 정의한다. 그 중 둘은 캐시 재검사를 할 때 가장 유용한 If-Modified-Since와 If-None-Match이다. 모든 조건부 헤더는 ‘If-‘ 접두어로 시작한다.

45. 강한 엔터티 태그는 대응하는 엔터티 값이 어떻게 바뀌든 매번 반드시 같이 바뀌어야 한다. 약한 엔터티 태그는 대응하는 엔터티에 유의미한 변경이 있을 때마다 같이 변경되어야 한다.

46. 웹브라우저는 브라우저나 프락시 캐시의 신선하지 않은 콘텐츠를 강제로 갱신시켜주는 리프레시나 리로드 버튼을 갖고 있다. 이 리프레시 버튼을 Cache-control 요청 헤더가 추가된 GET 요청을 발생시켜서, 강제로 재검사하거나 서버로부터 콘텐츠를 무조건 가져온다. 정확한 리프레시 동작은 각 브라우저나 문서, 중간 캐시 설정에 달려있다.

### 8. 통합점 : 게이트웨이, 터널, 릴레이

47. 게이트웨이  
HTTP의 확장과 인터페이스는 사람들의 필요에 따라 발전해왔다. 웹에 더 복잡한 리소스를 올려야 할 필요가 생기면서, 모든 리소스를 한 개의 어플리케이션으로만 처리할 수 없다는 것은 분명해졌다. 개발자들은 이 문제에 대한 해결책으로, 인터프리터 같이 리소스를 받기 위한 경로를 안내하는 역할을 하는 게이트웨이를 고안해냈다. 게이트웨이는 리소스와 애플리케이션을 연결하는 역할을 한다. 애플리케이션은 게이트웨이에게 요청을 처리해달라고 할 수 있고(HTTP 혹은 그 밖의 정의해 둔 인터페이스를 통해), 게이트웨이는 그에 응답할 수 있다. 게이트웨이는 요청을 받고 응답을 보내는 포털 같이 동작하는데, 동적인 콘텐츠를 생성하거나 데이터베이스에 질의를 보낼 수 있다.

48. 웹 터널을 사용하면 HTTP 커넥션을 통해서 HTTP가 아닌 트래픽을 전송할 수 있고, 다른 프로토콜을 HTTP 위에 올릴 수 있다. 웹 터널을 사용하는 가장 일반적인 이유는 HTTP 커넥션 안에 HTTP가 아닌 트래픽을 얹기 위해서다. 따라서 웹 터널을 사용하면 웹 트래픽만을 허락하는 방화벽이 있더라도 HTTP가 아닌 트래픽을 전송할 수 있다.

### 9. 웹 로봇
49. 크롤러와 크롤링  
웹 크롤러는, 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹페이지들을 가져오는, 이러한 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다. 웹 링크를 재귀적으로 따라가는 로봇을 크롤러 혹은 스파이더라고 부르는데, HTML 하이퍼링크들로 만들어진 웹을 따라 ‘기어다니기(crawl)’ 때문이다.

50. 웹 사이트의 어떤 URL을 방문하기 전에, 그 웹 사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 한다. 호스트 명과 포트번호에 의해 정의되는 어떤 웹 사이트가 있을 때, 그 사이트 전체에 대한 robots.txt 파일은 단 하나만이 존재한다. 만약 웹 사이트가 가상 호스팅된다면, 다른 모든 파일이 그러하듯이 각각의 가상 docroot에 서로 다른 robots.txt가 있을 수 있다.

51. 많은 웹페이지가 주어진 단어를 포함할 수 있기 때문에, 검색엔진은 결과에 순위를 매기기 위해 똑똑한 알고리즘을 사용한다. 예를 들어, 그림 9-8에서는 복수 개의 문서에 단어 ‘best’가 나타나고 있다. 검색엔진은 그 문서들이 주어진 단어와 가장 관련이 많은 순서대로 결과 문서에 나타날 수 있도록 문서들 간의 순서를 알 필요가 있다. 이것은 관련도 랭킹(relevancy ranking)이라고 불리며, 검색 결과의 목록에 점수를 매기고 정렬하는 과정이다.
 이 과정을 더 잘 지원하기 위해, 많은 검색엔진이 웹을 크롤링하는 과정에서 수집된 통계 데이터를 실제로 사용한다. 예를 들어, 어떤 주어진 페이지를 가리키는 링크들이 얼마나 많은지 세는 것은 그 문서의 인기도를 판별하는데 도움이 된다. 그리고 이 정보는 결과를 보여줄 때 정렬 순서에 대한 가중치로 사용될 수 있다. 검색엔진에 의해 사용되는 알고리즘, 크롤링에 대한 팁, 그 외 각종 기교는 검색엔진의 가장 엄격히 감추어진 비밀들이다.

### 10. HTTP/2.0
52. HTTP/2.0의 등장 배경  
HTTP/1.1의 메시지 포맷은 구현의 단순성과 접근성에 주안점을 두고 최적화되었다. 그러다 보니 성능은 어느 정도 희생시키지 않을 수 없었다. 커넥션 하나를 통해 요청 하나를 보내고 그에 대해 응답 하나만을 받는 HTTP의 메시지 교환 방식은 단순함 면에서는 더할 나위 없었지만, 응답을 받아야만 그다음 요청을 보낼 수 있기 때문에 심각한 회전 지연(latency)를 피할 수 없었다. 이 문제를 회피하기 위해 병렬 커넥션이나 파이프라인 커넥션이 도입되었지만 성능 개선에 대한 근본적인 해결책은 되지 못했다.

53. 그러나 HTTP/2.0에서는 하나의 커넥션에 여러 개의 스트림이 동시에 열릴 수 있다. 따라서 하나의 HTTP/2.0 커넥션을 통해 여러 개의 요청이 동시에 보내질 수 있기 때문에 이 문제는 쉽게 해결될 수 있다.
 뿐만 아니라 스트림은 우선순위도 가질 수 있다. 예를 들어 사용자가 웹브라우저로 어떤 웹페이지를 보내려고 할 때, 네트워크 대역폭이 충분하지 않아 프레임의 전송이 느리다면, 웹브라우저는 보다 중요한 리소스(예를 들면 이미지 파일보다는 HTML 페이지)를 요청하는 스트림에게 더 높은 우선순위를 부여할 수 있을 것이다. 그러나 이 우선순위에 따르는 것은 의무사항이 아니기 때문에, 요청이 우선순위대로 처리된다는 보장은 없다.

54. 서버 푸시  
HTTP/2.0은 서버가 하나의 요청에 대해 응답으로 여러 개의 리소스를 보낼 수 있도록 해준다. 이 기능은 서버가 클라이언트에서 어떤 리소스를 요구할 것인지 미리 알 수 있는 상황에서 유용하다. 예를 들어, HTML 문서를 요청 받은 서버는 그 HTML 문서가 링크하고 있는 이미지, CSS 파일, 자바스크립트 파일 등의 리소스를 클라이언트에게 푸시할 수 있을 것이다. 이는 클라이언트가 HTML 문서를 파싱해서 필요한 리소스를 다시 요청하여 발생하게 되는 트래픽과 회전 지연을 줄여준다.

# Part 3. 식별, 인가, 보안
### 11. 클라이언트 식별과 쿠키
55. 쿠키의 타입  
쿠키는 크게 세션 쿠키(session cookie)와 지속 쿠키(persistent cookie) 두 가지 타입으로 나눌 수 있다. 세션 쿠키는 사용자가 사이트를 탐색할 때, 관련한 설정과 선호 사항들을 저장하는 임시 쿠키다. 세션 쿠키는 사용자가 브라우저를 닫으면 삭제된다. 지속 쿠키는 삭제되지 않고 더 길게 유지될 수 있다. 지속 쿠키는 디스크에 저장되어, 브라우저를 닫거나 컴퓨터를 재시작하더라도 남아있다. 지속 쿠키는 사용자가 주기적으로 방문하는 사이트에 대한 설정 정보나 로그인 이름을 유지하려고 사용한다.

### 12. 기본 인증
56. 기본 인증은 가장 잘 알려진 HTTP 인증 규약이다. 거의 모든 주요 클라이언트와 서버에 기본 인증이 구현되어 있다. 기본 인증은 원래 HTTP/1.0에 기술되어 있었지만, HTTP 인증의 상세 내용을 다루는 RFC 2617로 옮겨졌다.
 기본 인증에서, 웹 서버는 클라이언트의 요청을 거부하고 유효한 사용자 이름과 비밀번호를 요구할 수 있다. 서버는 200 대신 401 상태 코드와 함께, 클라이언트가 접근하려고 했던 보안 영역을 WWW-Authenticate에 기술해서 응답하여 인증요구를 시작한다. 인증 정보를 포함하여 요청하라는 응답을 받은 브라우저는, 사용자에게 게정과 비밀번호를 입력할 수 있는 대화상자를 연다. 여기서 계정과 비밀번호는 사용자가 해당 영역에 접근 권한이 있는지 검사하는데 사용된다. 브라우저는 사용자가 입력한 사용자 이름과 비밀번호를 Authorization 요청 헤더 안에 암호화해서 서버로 다시 보낸다.

57. Base-64 인코딩은 바이너리, 텍스트, 국제 문자 데이터(어떤 시스템에서는 문제를 일으킬 수 있는) 문자열 받아서 전송할 수 있게, 그 문자열을 전송 가능한 문자인 알파벳으로 변환하기 위해 발명됐다. 전송 중에 원본 문자열이 변질될 걱정 없이 원격에서 디코딩할 수 있다.
 Base-64 인코딩은 국제 문자나 HTTP 헤더에서 사용할 수 없는 문자(큰따옴표, 콜론, 캐리지 리턴)를 포함한 사용자 이름이나 비밀번호를 보내야 할 때 유용할 수 있다. 또한, base-64는 어렵지 않게 사용자 이름과 비밀번호 문자를 섞을 수 있기 때문에, 서버나 네트워크를 관리하면서 뜻하지 않게 사용자 이름과 비밀번호가 노출되는 문제를 예방하는 데 도움이 된다.

58. base-64로 인코딩된 비밀번호는 사실상 ‘비밀번호 그대로’ 보내는 것과 다름없다.

### 13. 다이제스트 인증

### 14. 보안 HTTP
59.  
- 서버 인증 : 클라이언트는 자신이 위조된 서버가 아닌 진짜와 이야기하고 있음을 알 수 있어야 한다.
- 클라이언트 인증 : 서버는 자신이 가짜가 아닌 진짜 사용자와 이야기하고 있음을 알 수 있어야 한다.
- 무결성 : 클라이언트와 서버는 그들의 데이터가 위조되는 것으로부터 안전해야 한다.
- 암호화 : 클라이언트와 서버는 도청에 대한 걱정 없이 서로 대화할 수 있어야 한다.
- 효율 : 저렴한 클라이언트나 서버도 이용할 수 있도록 알고리즘은 충분히 빨라야 한다.
- 편재성(Ubiquity) : 프로토콜은 거의 모든 클라이언트와 서버에서 지원되어야 한다.
- 관리상 확장성 : 누구든 어디서든 즉각적인 보안 통신을 할 수 있어야 한다.
- 적응성 : 현재 알려진 최선의 보안 방법을 지원해야 한다.
- 사회적 생존성 : 사회의 문화적, 정치적 요구를 만족시켜야 한다.

60. HTTPS를 사용할 때, 모든 HTTP 요청과 응답 데이터는 네트워크로 보내지기 전에 암호화된다. HTTPS는 HTTP의 하부에 전송 레벨 암호 보안 계층을 제공함으로써 동작하는데, 이 보안 계층은 안전 소켓 계층(Secure Sockets Layer, SSL) 혹은 그를 계승한 전송 계층 보안(Transport Layer Security, TLS)을 이용하여 구현된다.

61. 키가 있는 암호  
코드 알고리즘과 기계가 적에 손에 들어갈 수 있기 때문에, 대부분의 기계들에는 암호의 동작방식을 변경할 수 있는 큰 숫자로 된 다른 값을 설정할 수 있는 다이얼이 달려있다. 누군가 기계를 훔치더라도, 올바른 다이얼 설정(키 값)이 없이는 디코더가 동작하지 않을 것이다.

62. 공개키 암호법  
한 쌍의 호스트가 하나의 인코딩/디코딩 키를 사용하는 대신, 공개키 암호 방식은 두 개의 비대칭 키를 사용한다. 하나는 호스트의 메시지를 인코딩하기 위한 것이며, 다른 하나는 그 호스트의 메시지를 디코딩하기 위한 것이다. 인코딩 키는 모두를 위해 공개되어 있다(그래서 공개키 암호 방식이라는 이름이 붙었다). 하지만 호스트만이 개인 디코딩 키를 알고 있다.
 노드 X는 자신의 인코딩 키 e^x를 공개적으로 배포할 수 있다. 이제 메시지를 노드 X에게 보내고자 하는 누구나 똑같고 잘 알려진 공개키를 사용할 수 있다. 각 호스트마다 누구나 사용할 수 있는 인코딩 키가 할당되어 있기 때문에, 공개키 암호 방식은 대칭 키의 쌍이 N^2로 폭발적으로 증가하는 것을 피할 수 있다.
 모든 사람이 X에게 보내는 메시지를 같은 키로 인코딩 할 수 있지만, X를 제외한 누구도 그 메시지를 디코딩할 수 없다. 왜냐하면 오직 X만이 디코딩 개인 키 d^X를 갖고 있기 때문이다. 키의 분리는, 메시지의 인코딩은ㅇ 누구나 할 수 있도록 해주는 동시에, 메시지를 디코딩하는 능력은 소유자에게만 부여한다. 이는 노드가 서버로 안전하게 메시지를 발송하는 것을 더 쉽게 해주는데, 왜냐하면 서버의 공개 키만 있으면 되기 때문이다.

63. HTTPS 개요  
HTTPS는 그냥 보안 전송 계층을 통해 전송되는 HTTP이다. 암호화되지 않은 HTTP 메시지를 TCP를 통해 전 세계의 인터넷 곳곳으로 보내는 대신에, HTTPS는 HTTP 메시지를 TCP로 보내기 전에 먼저 그것들을 암호화하는 보안 계층으로 보낸다.

64. SSL 핸드셰이크  
암호화된 HTTP 메시지를 보낼 수 있게 되기 전에, 클라이언트와 서버는 SSL 핸드셰이크를 할 필요가 있다. 핸드셰이크에서는 다음과 같은 일이 일어난다.
- 프로토콜 버전 번호 교환
- 양쪽이 알고 있는 암호 선택
- 양쪽의 신원을 인증
- 채널을 암호화하기 위한 임시 세션 키 생성

# Part 4. 엔터티, 인코딩, 국제화
### 15. 엔터티와 인코딩
65. HTTP/1.1은 다음과 같이 10가지 주요 엔터티 헤더 필드를 정의하였다.
- Content-Type : 엔터티에 의해 전달된 객체의 종류
- Content-Length : 전달되는 메시지의 길이나 크기
- Content-Language : 전달되는 객체와 가장 잘 대응되는 자연어
- Content-Encoding : 객체 데이터에 대해 행해진 변형(압축 등)
- Content-Location : 요청 시점을 기준으로, 객체의 또 다른 위치
…

66. 콘텐츠 인코딩  
HTTP는 보안을 강화하거나 압축을 통해 공간을 절약할 수 있도록, 엔터티 본문을 인코딩할 수 있게 해준다. 만약 본문의 콘텐츠가 인코딩되어 있다면, Content-Length 헤더는 인코딩되지 않은 원본의 길이가 아닌 인코딩된 본문의 길이를 바이트 단위로 정의한다.

67. 엔터티 요약  
HTTP가 일반적으로 TCP/IP와 같이 신뢰할 만한 전송 프로토콜 위에서 구현됨에도 불구하고, 불완전한 트랜스코딩 프락시나 버그 많은 중개자 프락시를 비롯한 여러가지 이유로 메시지의 일부분이 전송 중에 변형되는 일이 일어난다. 엔터티 본문 데이터에 대한 의도하지 않은(혹은 달갑지 않은) 변경을 감지하기 위해, 최초 엔터티가 생성될 때 송신자는 데이터에 대한 체크섬을 생성할 수 있으며, 수신자는 모든 의도하지 않은 엔터티의 변경을 잡아내기 위해 그 체크섬으로 기본적인 검사를 할 수 있다.

68. Content-Type의 값은 인터넷 할당 번호 관리기관(Internet Assigned Numbers Authority, IANA)에 등록된 표준화된 MIME 타입이다. MIME 타입은 주 미디어 타입(텍스트, 이미지, 오디오 등)으로 시작해서 뒤이어 빗금(/), 그리고 미디어 타입을 더 구체적으로 서술하는 부 타입(subtype)으로 구성된다.

69. Transfer-Encoding  
안전한 정송을 위해 어떤 인코딩이 메시지에 적용되었는지 수신자에게 알려준다.

70. 청크 인코딩  
청크 인코딩은 메시지를 일정 크기의 청크 여럿으로 쪼갠다. 서버는 각 청크를 순차적으로 보낸다. 청크 인코딩을 이용하면 메시지를 보내기 전에 전체 크기를 알 필요가 없어진다. 본문이 동적으로 생성됨에 따라, 서버는 그중 일부를 버퍼에 담은 뒤 그 한 청크를 그것의 크기와 함께 보낼 수 있다. 본문 전체를 모두 보낼 때까지 이 단계를 반복한다.

### 16. 국제화

### 17. 내용 협상과 트랜스코딩
71. 내용 협상 기법  
서버에 있는 페이지들 중 어떤 것이 클라이언트에게 맞는지 판단하는 세 가지 다른 방법이 있다. 클라이언트에게 선택지를 주거나 서버가 자동으로 판단하는 방법, 혹은 중개자에게 선택하도록 부탁하는 방법. 이 세 가지 기법은 각각 클라이언트 주도 협상, 서버 주도 협상, 그리고 투명한 협상이라고 불린다.

# Part 5. 콘텐츠 발행 및 배포
### 18. 웹 호스팅
### 19. 배포 시스템
### 20. 리다이렉션과 부하 균형
### 21. 로깅과 사용 추적
72. 로그란 무엇인가?  
대개 로깅을 하는 이유는 두 가지다. 서버나 프락시의 문제를 찾거나, 웹 사이트 접근 통계를 내려고 로깅을 한다. 통계는 마케팅, 청구, 장비 조달 계획(예를 들어, 서버나 대역폭을 늘릴 필요가 있는지 결정하려고)을 세우는 데 유용하다.

73. 보통 트랜잭션의 기본적인 항목들만 로깅한다. 일반적으로 로깅하는 필드는 다음과 같은 것들이다.
- HTTP 메서드
- 클라이언트와 서버의 HTTP 버전
- 요청받은 리소스의 URL
- 응답의 HTTP 상태 코드
- 요청과 응답 메시지의 크기(모든 엔터티 본문을 포함)
- 트랜잭션이 일어난 시간
- Referer와 User-Agent 헤더 값

# Part 6. 부록
### 부록 A. URI 스킴
### 부록 B. HTTP 상태 코드
### 부록 C. HTTP 헤더 레퍼런스
### 부록 D. MIME 타입
### 부록 E. base-64 인코딩
### 부록 F. 다이제스트 인증
### 부록 G. 언어 태그
### 부록 H. MIME 문자집합 등록
